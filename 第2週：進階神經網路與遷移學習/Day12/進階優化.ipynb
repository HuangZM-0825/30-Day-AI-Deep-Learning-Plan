{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第十二天的學習計劃中，我們將討論一些 **進階優化技術**，如 **權重衰減（Weight Decay）** 和 **學習率調整（Learning Rate Scheduling）**，這些技術有助於提升模型的性能並防止過擬合。這些技術已被廣泛應用於現代深度學習模型的訓練中。\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 權重衰減（Weight Decay）**\n",
    "\n",
    "#### **1.1 權重衰減的概念**\n",
    "\n",
    "**權重衰減** 是一種 **正則化技術**，通過在損失函數中引入權重的懲罰項來防止模型過擬合。這種技術可以限制模型中的權重變得過大，從而提升模型的泛化能力。權重衰減通常是 L2 正則化的一種形式，其作用是在優化過程中同時最小化損失和權重的平方和。\n",
    "\n",
    "#### **1.2 權重衰減的數學表達**\n",
    "\n",
    "損失函數可以表示為：\n",
    "\n",
    "$$\n",
    "L_{\\text{new}} = L_{\\text{old}} + \\lambda \\sum_{i} w_i^2\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $L_{\\text{old}}$ 是原始損失函數（如交叉熵損失）。\n",
    "- $w_i$ 是模型中的權重參數。\n",
    "- $\\lambda$ 是控制正則化強度的超參數，稱為 **正則化係數**。\n",
    "\n",
    "#### **1.3 權重衰減的應用場景**\n",
    "\n",
    "- **過擬合的防止**：當模型過於擬合訓練數據時，可以通過引入權重衰減來約束模型，避免它學到不必要的細節。\n",
    "- **增強模型的穩定性**：權重衰減可以減少模型在訓練過程中權重的波動性，從而提高模型的穩定性。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 學習率調整（Learning Rate Scheduling）**\n",
    "\n",
    "#### **2.1 學習率調整的概念**\n",
    "\n",
    "**學習率調整** 是指在訓練過程中根據預定的策略動態調整學習率。使用固定學習率訓練模型可能會導致模型無法快速收斂或難以到達最優解，因此，動態調整學習率可以更有效地幫助模型在不同階段進行更優的更新。\n",
    "\n",
    "#### **2.2 常見的學習率調整策略**\n",
    "\n",
    "- **Step Decay（階梯衰減）**：每隔一定的 epoch，將學習率減少一個固定比例。\n",
    "  - 公式：\n",
    "    $$\n",
    "    \\eta_t = \\eta_0 \\cdot \\text{factor}^{\\left(\\frac{t}{\\text{decay\\_steps}}\\right)}\n",
    "    $$\n",
    "\n",
    "- **Exponential Decay（指數衰減）**：學習率隨著訓練次數的增加按指數方式衰減。\n",
    "  - 公式：\n",
    "    $$\n",
    "    \\eta_t = \\eta_0 \\cdot e^{-\\lambda t}\n",
    "    $$\n",
    "\n",
    "- **Cosine Annealing（餘弦退火）**：學習率根據餘弦函數進行退火，這樣可以實現學習率在訓練過程中周期性波動。\n",
    "  - 公式：\n",
    "    $$\n",
    "    \\eta_t = \\eta_{\\min} + \\frac{1}{2} (\\eta_0 - \\eta_{\\min}) \\left(1 + \\cos\\left(\\frac{t}{T_{\\max}} \\pi\\right)\\right)\n",
    "    $$\n",
    "\n",
    "- **ReduceLROnPlateau**：當模型的性能（如驗證損失）在若干 epoch 內沒有改善時，減少學習率。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 總結**\n",
    "\n",
    "- **權重衰減（Weight Decay）** 是一種有效的正則化技術，能夠防止模型過擬合，通過控制權重的大小來提高模型的泛化能力。\n",
    "- **學習率調整（Learning Rate Scheduling）** 是通過動態改變學習率來加速模型收斂或防止收斂到局部最小值。常見的學習率調整方法包括階梯衰減、指數衰減和餘弦退火。\n",
    "- 這些技術在 Keras 和 PyTorch 中都很容易實作，並且對模型性能有顯著的提升作用。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
