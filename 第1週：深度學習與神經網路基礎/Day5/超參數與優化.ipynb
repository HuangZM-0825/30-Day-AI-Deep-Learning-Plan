{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第五天的學習中，我們將討論深度學習中一些重要的 **超參數**，包括 **學習率（Learning Rate）**、**批量大小（Batch Size）** 以及不同的 **優化器（如 SGD、Adam）**。這些超參數的選擇對模型的訓練效果和收斂速度有著至關重要的影響。\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **學習率（Learning Rate）**\n",
    "\n",
    "**學習率** 是深度學習中最重要的超參數之一，決定了每次權重更新時步長的大小。學習率過大或過小都會影響模型的性能。\n",
    "\n",
    "#### **1.1 學習率的影響**\n",
    "- **學習率太大：** 如果學習率過大，模型可能會在損失函數的最小值附近來回震盪，甚至無法收斂，因為每次更新都跨越了最優點。\n",
    "- **學習率太小：** 如果學習率過小，模型收斂速度非常慢，訓練時間過長，有時甚至可能陷入局部最小值。\n",
    "\n",
    "#### **1.2 學習率調整方法**\n",
    "- **學習率衰減（Learning Rate Decay）：** 在訓練過程中逐漸減少學習率，使得模型在訓練初期快速靠近最優值，後期精細調整，最終達到穩定狀態。\n",
    "  - **指數衰減（Exponential Decay）：**\n",
    "    $$\n",
    "    \\eta_t = \\eta_0 \\cdot \\exp(-\\lambda t)\n",
    "    $$\n",
    "    其中 $\\eta_t$ 是第 $t$ 次迭代的學習率，$\\eta_0$ 是初始學習率，$\\lambda$ 是衰減率。\n",
    "  \n",
    "- **預熱（Learning Rate Warmup）：** 在訓練初期，逐漸增大學習率，以防止模型在初期陷入局部最小值。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **批量大小（Batch Size）**\n",
    "\n",
    "**批量大小** 決定了每次迭代更新權重時，使用的訓練樣本數量。根據不同的批量大小，訓練可以分為三種類型：\n",
    "\n",
    "#### **2.1 批量大小的類型**\n",
    "- **批量梯度下降（Batch Gradient Descent）：** 使用整個訓練集來計算一次梯度更新。\n",
    "  - **優點：** 每次更新的梯度方向準確。\n",
    "  - **缺點：** 計算成本高，對內存要求大。\n",
    "  \n",
    "- **小批量梯度下降（Mini-batch Gradient Descent）：** 使用一小部分樣本（如 32、64）來計算每次梯度更新。\n",
    "  - **優點：** 兼顧計算效率和更新穩定性，是目前最常用的訓練方式。\n",
    "  - **缺點：** 梯度估計有少許噪聲，但相對穩定。\n",
    "\n",
    "- **隨機梯度下降（Stochastic Gradient Descent, SGD）：** 每次只使用一個樣本來計算梯度更新。\n",
    "  - **優點：** 訓練速度快，適合大數據集的在線學習。\n",
    "  - **缺點：** 梯度估計噪聲較大，收斂不穩定。\n",
    "\n",
    "#### **2.2 批量大小的選擇**\n",
    "- 小批量（如 32、64、128）通常是較好的選擇。這種方式能在內存佔用和計算效率之間取得平衡，並且更新過程足夠穩定。\n",
    "\n",
    "#### **2.3 批量大小對模型的影響**\n",
    "- **大批量：** 模型更新更加穩定，但計算成本高，對內存要求大。\n",
    "- **小批量：** 訓練更快，但更新過程中可能會引入較多噪聲。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **優化器（Optimizer）**\n",
    "\n",
    "優化器是用來更新神經網路中的權重的算法，它的選擇對模型的性能有很大的影響。不同的優化器有不同的權重更新策略。\n",
    "\n",
    "#### **3.1 隨機梯度下降（SGD, Stochastic Gradient Descent）**\n",
    "\n",
    "**SGD** 是最基本的優化算法，每次只使用一個樣本來計算梯度並更新權重。SGD 的更新規則為：\n",
    "$$\n",
    "w = w - \\eta \\cdot \\nabla_w L\n",
    "$$\n",
    "其中：\n",
    "- $w$ 是模型的權重，\n",
    "- $\\eta$ 是學習率，\n",
    "- $\\nabla_w L$ 是損失函數對權重的偏導數。\n",
    "\n",
    "#### **SGD 的優缺點：**\n",
    "- **優點：** 計算速度快，對於大數據集的在線學習有很好的效果。\n",
    "- **缺點：** 由於每次更新的樣本不同，梯度估計存在噪聲，收斂過程不穩定，並且容易陷入局部最小值。\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.2 動量梯度下降（Momentum）**\n",
    "\n",
    "**動量（Momentum）** 是對 SGD 的改進，它引入了一個動量項，幫助加速收斂，並減少梯度更新的波動性。動量的更新規則為：\n",
    "$$\n",
    "v_t = \\gamma v_{t-1} + \\eta \\cdot \\nabla_w L\n",
    "$$\n",
    "$$\n",
    "w = w - v_t\n",
    "$$\n",
    "其中：\n",
    "- $v_t$ 是動量，$\\gamma$ 是動量系數（通常設定為 $0.9$），表示當前方向受到前幾次更新的影響。\n",
    "\n",
    "#### **動量的優點：**\n",
    "- 能夠加速收斂，並減少梯度更新的震盪。\n",
    "\n",
    "---\n",
    "\n",
    "#### **3.3 Adam 優化器（Adaptive Moment Estimation）**\n",
    "\n",
    "**Adam** 是目前最常用的優化算法之一，它結合了動量和 RMSProp（自適應學習率）兩種優化算法的優點。Adam 的更新規則如下：\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_w L\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_w L)^2\n",
    "$$\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "$$\n",
    "w = w - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "$$\n",
    "其中：\n",
    "- $m_t$ 是梯度的一階動量（即梯度的移動平均），$v_t$ 是梯度的二階動量（即梯度平方的移動平均），\n",
    "- $\\beta_1$ 和 $\\beta_2$ 是動量係數，$\\epsilon$ 是一個很小的常數，防止分母為零。\n",
    "\n",
    "#### **Adam 的優點：**\n",
    "- 自適應學習率能夠自動調整每個參數的學習步長，適合不同類型的數據集。\n",
    "- 收斂速度快，對於大多數深度學習任務表現良好。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **總結與建議**\n",
    "\n",
    "- **學習率：** 選擇合適的學習率非常關鍵，太大會導致訓練不穩定，太小則訓練時間過長。通常可以通過學習率衰減或預熱策略來優化。\n",
    "- **批量大小：** 小批量（如 32、64、128）是常見的選擇，能夠在效率和穩定性之間取得平衡。\n",
    "- **優化器：** Adam 是目前最常用的優化器，適合大多數任務；SGD 和動量適合更靈活的調整和較簡單的任務。\n",
    "\n",
    "透過調整這些超參數，你可以優化模型的訓練效果，達到更好的性能。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
