{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第四天的學習計畫中，我們將專注於 **優化 CNN 模型**，並介紹 **池化（Pooling）** 和 **Dropout** 等正則化技術，以提高模型的性能，並防止過擬合。\n",
    "\n",
    "### 1. **CNN 模型的優化**\n",
    "\n",
    "在訓練 CNN 時，我們可以通過一些技術和策略來提升模型的表現，主要的優化方法包括以下幾個方面：\n",
    "\n",
    "#### **1.1 增加卷積層的深度**\n",
    "增加 CNN 的深度可以幫助模型學習更多層次的特徵，例如低層次的邊緣特徵和高層次的物體特徵。但要注意避免過深的模型導致訓練時間過長或過擬合。\n",
    "\n",
    "#### **1.2 調整卷積核大小**\n",
    "小卷積核（如 $3 \\times 3$）可以捕捉到更多的細節特徵，而大卷積核（如 $5 \\times 5$）可以更快地捕捉到整體的輪廓。通常使用小卷積核疊加多層是比較常見的做法，例如 VGGNet 使用多個 $3 \\times 3$ 的卷積核堆疊來替代大卷積核。\n",
    "\n",
    "#### **1.3 使用適當的學習率**\n",
    "選擇合適的學習率對於模型訓練的收斂速度和效果至關重要。如果學習率太大，模型可能無法收斂；如果學習率太小，模型的訓練速度會非常慢。可以使用學習率衰減來逐漸減少學習率，這有助於模型在訓練後期達到更好的結果。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **池化（Pooling）技術**\n",
    "\n",
    "池化層的主要作用是對特徵圖進行降維，從而減少參數數量、計算量，並防止過擬合。此外，池化還有助於捕捉更大的感受野。\n",
    "\n",
    "#### **2.1 最大池化（Max Pooling）**\n",
    "最大池化是 CNN 中最常用的池化方法，它在池化窗口內選取最大值，這有助於保留圖像中最顯著的特徵。\n",
    "\n",
    "- **公式：**\n",
    "  $$\n",
    "  y = \\max(x_1, x_2, \\dots, x_n)\n",
    "  $$\n",
    "\n",
    "- **作用：** 最大池化可以幫助模型保留圖片中最具代表性的特徵，例如邊緣和角點，並有效減少輸出特徵圖的尺寸。\n",
    "\n",
    "#### **2.2 平均池化（Average Pooling）**\n",
    "平均池化對池化窗口內的所有值進行平均，它能夠保留更多的背景信息，但通常最大池化效果更好，因此平均池化較少使用。\n",
    "\n",
    "- **公式：**\n",
    "  $$\n",
    "  y = \\frac{1}{N} \\sum_{i=1}^{N} x_i\n",
    "  $$\n",
    "\n",
    "- **作用：** 平均池化適合在某些需要保留全局背景信息的任務中使用，但在大多數 CNN 應用中，最大池化被更廣泛地使用。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Dropout 技術**\n",
    "\n",
    "**Dropout** 是一種有效的正則化技術，用來防止神經網路中的過擬合問題。過擬合是指模型在訓練數據上表現良好，但在測試數據上表現不佳，這是由於模型過度記住了訓練數據的細節。\n",
    "\n",
    "#### **3.1 Dropout 的工作原理**\n",
    "- 在訓練期間，Dropout 隨機地關閉神經網路中的某些神經元，使得網路中的不同子網絡進行訓練，這有助於提升模型的泛化能力。\n",
    "  \n",
    "- 在測試期間，所有神經元都處於激活狀態，並且神經元的輸出按訓練過程中被關閉的比例縮放，以保證預測結果的一致性。\n",
    "\n",
    "#### **3.2 Dropout 的數學表達**\n",
    "假設神經元輸出 $z$，Dropout 的激活機制為：\n",
    "$$\n",
    "z' = z \\cdot r\n",
    "$$\n",
    "其中 $r$ 是一個隨機變量，$r = 0$ 的概率為 $p$，$r = 1$ 的概率為 $1 - p$。也就是說，$p$ 是關閉神經元的比例。\n",
    "\n",
    "#### **3.3 Dropout 的優點**\n",
    "- **防止過擬合：** Dropout 使得每次訓練都會使用不同的神經元組合，這能有效地避免模型過度記住訓練數據中的細節。\n",
    "- **提升泛化能力：** Dropout 使得模型對於新數據的預測更加穩定。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **其他正則化技術**\n",
    "\n",
    "#### **4.1 L2 正則化**\n",
    "L2 正則化會在損失函數中加入權重的平方和，這樣模型的權重就不會變得過大，從而防止過擬合。L2 正則化的損失函數表達如下：\n",
    "$$\n",
    "L_{\\text{new}} = L_{\\text{old}} + \\lambda \\sum w^2\n",
    "$$\n",
    "其中，$\\lambda$ 是正則化係數，決定了正則化項的權重大小。\n",
    "\n",
    "#### **4.2 批量正規化（Batch Normalization）**\n",
    "批量正規化能加快模型的收斂速度，並減少過擬合風險。它通過在每一層的輸出上進行標準化來控制輸出分佈的偏移，這樣可以使得每層輸出的分佈更加穩定，減少梯度消失或爆炸的風險。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **總結與 CNN 優化建議**\n",
    "\n",
    "- **池化（Pooling）：** 使用最大池化來減少特徵圖尺寸，並有效保留關鍵特徵。\n",
    "- **Dropout：** 在全連接層等容易過擬合的部分使用 Dropout 技術，防止模型記住過多訓練數據的細節。\n",
    "- **正則化技術：** 使用 L2 正則化來控制權重大小，使用批量正規化來加速收斂並提高模型穩定性。\n",
    "\n",
    "這些技術能夠幫助 CNN 模型在實際應用中提升性能，尤其是在面對較大的數據集或深層網路時，它們能有效防止過擬合問題，並提高模型的泛化能力。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
