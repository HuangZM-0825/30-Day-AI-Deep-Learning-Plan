{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷積神經網路（Convolutional Neural Network，CNN）**，說明背景、結構、各個組成部分的功能，以及它們如何在圖像資料中應用。\n",
    "\n",
    "---\n",
    "\n",
    "### **卷積神經網路（CNN）詳解**\n",
    "\n",
    "#### **1. 背景與動機**\n",
    "\n",
    "傳統的前饋神經網路（如多層感知器，MLP）在處理高維度的圖像數據時，面臨以下挑戰：\n",
    "\n",
    "- **參數過多：** 全連接層的權重數量與輸入尺寸相關，圖像通常具有大量像素，導致參數爆炸。\n",
    "- **空間信息損失：** 將圖像展平成一維向量後，失去了像素之間的空間關係。\n",
    "- **過擬合風險：** 過多的參數容易導致模型對訓練數據過擬合。\n",
    "\n",
    "為了解決這些問題，CNN 應運而生，利用圖像的**局部性**和**平移不變性**特性，通過卷積操作提取有效特徵，減少參數數量，提高模型性能。\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. CNN 的核心概念**\n",
    "\n",
    "CNN 主要利用以下兩個關鍵思想：\n",
    "\n",
    "- **局部連接（Local Connectivity）：** 每個神經元只與上一層的局部區域相連，捕捉局部特徵。\n",
    "- **權重共享（Weight Sharing）：** 在同一卷積層中，所有位置使用相同的卷積核參數，降低了參數數量。\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. CNN 的主要組成部分**\n",
    "\n",
    "CNN 通常由以下幾種類型的層組成：\n",
    "\n",
    "1. **卷積層（Convolutional Layer）**\n",
    "2. **激活函數（Activation Function）**\n",
    "3. **池化層（Pooling Layer）**\n",
    "4. **正則化層（Regularization Layer）**\n",
    "5. **展開層（Flatten Layer）**\n",
    "6. **全連接層（Fully Connected Layer）**\n",
    "7. **輸出層（Output Layer）**\n",
    "\n",
    "---\n",
    "\n",
    "##### **3.1 卷積層（Convolutional Layer）**\n",
    "\n",
    "**功能：** 提取輸入數據的局部特徵。\n",
    "\n",
    "**原理：**\n",
    "\n",
    "- **卷積運算：** 卷積層通過學習一組**卷積核（Filter or Kernel）**，對輸入數據進行卷積操作，生成**特徵圖（Feature Map）**。\n",
    "\n",
    "- **數學表達：**\n",
    "\n",
    "  對於輸入特徵圖 $\\mathbf{X}$，卷積核 $\\mathbf{K}$，輸出特徵圖 $\\mathbf{S}$，卷積操作可表示為：\n",
    "\n",
    "  $$\n",
    "  S(i,j) = (\\mathbf{X} * \\mathbf{K})(i,j) = \\sum_{m}\\sum_{n} X(i+m,j+n) \\cdot K(m,n)\n",
    "  $$\n",
    "\n",
    "  其中：\n",
    "\n",
    "  - $X(i+m,j+n)$ 是輸入在位置 $(i+m,j+n)$ 的值。\n",
    "  - $K(m,n)$ 是卷積核在位置 $(m,n)$ 的值。\n",
    "  - $S(i,j)$ 是輸出特徵圖在位置 $(i,j)$ 的值。\n",
    "\n",
    "**特點：**\n",
    "\n",
    "- **局部感受野（Receptive Field）：** 卷積核的尺寸決定了每個神經元感知的範圍。\n",
    "- **參數共享：** 同一卷積核在整個輸入上進行卷積，參數共享降低了模型的複雜度。\n",
    "\n",
    "**超參數：**\n",
    "\n",
    "- **卷積核大小（Kernel Size）：** 如 $3 \\times 3$、$5 \\times 5$。\n",
    "- **卷積核數量（Number of Filters）：** 決定了輸出特徵圖的數量。\n",
    "- **步幅（Stride）：** 卷積核在輸入上滑動的步長，默認為 1。\n",
    "- **填充（Padding）：** 在輸入的邊界處填充額外的像素，控制輸出尺寸。\n",
    "\n",
    "---\n",
    "\n",
    "##### **3.2 激活函數（Activation Function）**\n",
    "\n",
    "**功能：** 引入非線性，使網路能夠學習複雜的模式。\n",
    "\n",
    "**常見的激活函數：**\n",
    "\n",
    "- **ReLU（Rectified Linear Unit）：**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\max(0, x)\n",
    "  $$\n",
    "\n",
    "- **Leaky ReLU：**\n",
    "\n",
    "  $$\n",
    "  f(x) = \\begin{cases} x, & \\text{if } x \\geq 0 \\\\ \\alpha x, & \\text{if } x < 0 \\end{cases}\n",
    "  $$\n",
    "\n",
    "  其中 $\\alpha$ 是一個很小的常數，例如 $0.01$。\n",
    "\n",
    "- **Sigmoid 函數：**\n",
    "\n",
    "  $$\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "\n",
    "- **Tanh 函數：**\n",
    "\n",
    "  $$\n",
    "  \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "  $$\n",
    "\n",
    "**選擇考慮：**\n",
    "\n",
    "- **ReLU** 計算簡單，收斂速度快，是目前最常用的激活函數。\n",
    "- **Leaky ReLU** 和 **Parametric ReLU** 等變體解決了 ReLU 的“死亡”問題（當輸入總是負數時，梯度為零）。\n",
    "\n",
    "---\n",
    "\n",
    "##### **3.3 池化層（Pooling Layer）**\n",
    "\n",
    "**功能：** 減少特徵圖的尺寸，降低計算量和過擬合風險。\n",
    "\n",
    "**類型：**\n",
    "\n",
    "- **最大池化（Max Pooling）：**\n",
    "\n",
    "  在池化窗口內取最大值。\n",
    "\n",
    "  $$\n",
    "  y = \\max_{(i,j) \\in \\text{window}} x_{i,j}\n",
    "  $$\n",
    "\n",
    "- **平均池化（Average Pooling）：**\n",
    "\n",
    "  在池化窗口內取平均值。\n",
    "\n",
    "  $$\n",
    "  y = \\frac{1}{N} \\sum_{(i,j) \\in \\text{window}} x_{i,j}\n",
    "  $$\n",
    "\n",
    "**超參數：**\n",
    "\n",
    "- **池化窗口大小（Pool Size）：** 如 $2 \\times 2$。\n",
    "- **步幅（Stride）：** 通常與池化窗口大小相同。\n",
    "\n",
    "---\n",
    "\n",
    "##### **3.4 正則化層（Regularization Layer）**\n",
    "\n",
    "**功能：** 防止過擬合，提高模型的泛化能力。\n",
    "\n",
    "**常見方法：**\n",
    "\n",
    "- **Dropout 層：**\n",
    "\n",
    "  以概率 $p$ 隨機將輸出設為零。\n",
    "\n",
    "  - **訓練時：** 隨機丟棄神經元輸出。\n",
    "  - **測試時：** 將輸出乘以 $1 - p$。\n",
    "\n",
    "---\n",
    "\n",
    "##### **3.5 展開層（Flatten Layer）**\n",
    "\n",
    "**功能：** 將多維的特徵圖展開為一維向量，以便輸入到全連接層。\n",
    "\n",
    "- **操作：**\n",
    "\n",
    "  將尺寸為 $(h, w, c)$ 的張量轉換為長度為 $h \\times w \\times c$ 的向量。\n",
    "\n",
    "---\n",
    "\n",
    "##### **3.6 全連接層（Fully Connected Layer, FC）**\n",
    "\n",
    "**功能：** 將前面提取的特徵進行分類或回歸。\n",
    "\n",
    "**數學表達：**\n",
    "\n",
    "- 對於輸入 $\\mathbf{x}$，權重矩陣 $\\mathbf{W}$，偏置 $\\mathbf{b}$，輸出為：\n",
    "\n",
    "  $$\n",
    "  \\mathbf{y} = f(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n",
    "  $$\n",
    "\n",
    "  其中 $f$ 是激活函數。\n",
    "\n",
    "**特點：**\n",
    "\n",
    "- **高層特徵融合：** 全連接層將不同位置的特徵進行綜合。\n",
    "\n",
    "---\n",
    "\n",
    "##### **3.7 輸出層（Output Layer）**\n",
    "\n",
    "**功能：** 產生最終的預測結果。\n",
    "\n",
    "- **分類問題：**\n",
    "\n",
    "  - **二分類：** 使用 Sigmoid 激活函數。\n",
    "\n",
    "    $$\n",
    "    \\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "    $$\n",
    "\n",
    "  - **多分類：** 使用 Softmax 激活函數。\n",
    "\n",
    "    $$\n",
    "    \\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "    $$\n",
    "\n",
    "    其中 $K$ 是類別數，$z_i$ 是第 $i$ 個輸出單元的輸入。\n",
    "\n",
    "- **回歸問題：** 輸出層不使用激活函數，直接輸出連續值。\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. CNN 的工作流程**\n",
    "\n",
    "1. **輸入圖像：** 原始數據，尺寸為 $(h, w, c)$，其中 $h$ 為高度，$w$ 為寬度，$c$ 為通道數。\n",
    "\n",
    "2. **卷積操作：** 通過多個卷積層提取不同層次的特徵。\n",
    "\n",
    "3. **激活函數：** 在每個卷積層後應用激活函數，引入非線性。\n",
    "\n",
    "4. **池化操作：** 定期插入池化層，縮小特徵圖尺寸。\n",
    "\n",
    "5. **正則化：** 使用 Dropout 等技術防止過擬合。\n",
    "\n",
    "6. **展開：** 將最終的特徵圖展開為一維向量。\n",
    "\n",
    "7. **全連接層：** 對展開的特徵進行進一步學習和分類。\n",
    "\n",
    "8. **輸出層：** 生成最終的預測結果。\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. CNN 的優勢**\n",
    "\n",
    "- **參數高效：** 利用局部連接和權重共享，大幅減少參數數量。\n",
    "\n",
    "- **捕捉空間結構：** 能夠有效地學習圖像中的空間層次特徵。\n",
    "\n",
    "- **平移不變性：** 具有對輸入圖像平移的魯棒性。\n",
    "\n",
    "- **自動特徵學習：** 無需手工設計特徵，直接從數據中學習。\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. CNN 的常見架構**\n",
    "\n",
    "- **LeNet-5（1998）：** 最早的 CNN，用於手寫字辨識。\n",
    "\n",
    "- **AlexNet（2012）：** 引入 ReLU、Dropout、大卷積核，奪得 ImageNet 冠軍。\n",
    "\n",
    "- **VGGNet（2014）：** 使用多個小卷積核（$3 \\times 3$）堆疊，提升深度。\n",
    "\n",
    "- **GoogLeNet（Inception）（2014）：** 引入 Inception 模塊，採用網路中網路結構。\n",
    "\n",
    "- **ResNet（2015）：** 引入殘差塊，解決深層網路的退化問題。\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. CNN 的應用**\n",
    "\n",
    "- **圖像分類**\n",
    "\n",
    "- **物件檢測（如 R-CNN、YOLO、SSD）**\n",
    "\n",
    "- **圖像分割（如 U-Net、Mask R-CNN）**\n",
    "\n",
    "- **圖像生成與風格遷移**\n",
    "\n",
    "- **視頻分析**\n",
    "\n",
    "- **自然語言處理（與 CNN 結合）**\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. 重要概念補充**\n",
    "\n",
    "##### **8.1 填充（Padding）**\n",
    "\n",
    "- **目的：** 控制卷積後特徵圖的尺寸，保留輸入邊緣的信息。\n",
    "\n",
    "- **類型：**\n",
    "\n",
    "  - **Valid 卷積：** 無填充，輸出尺寸減小。\n",
    "\n",
    "  - **Same 卷積：** 通過填充使輸出尺寸與輸入相同。\n",
    "\n",
    "- **計算公式：**\n",
    "\n",
    "  給定輸入尺寸 $N$，卷積核尺寸 $F$，填充 $P$，步幅 $S$，則輸出尺寸 $O$ 為：\n",
    "\n",
    "  $$\n",
    "  O = \\left\\lfloor \\frac{N + 2P - F}{S} \\right\\rfloor + 1\n",
    "  $$\n",
    "\n",
    "##### **8.2 步幅（Stride）**\n",
    "\n",
    "- **定義：** 卷積核在輸入上移動的步長。\n",
    "\n",
    "- **影響：** 增大步幅會減小輸出特徵圖的尺寸。\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. CNN 的訓練與優化**\n",
    "\n",
    "- **損失函數：**\n",
    "\n",
    "  - **分類問題：** 使用交叉熵損失。\n",
    "\n",
    "    $$\n",
    "    L = -\\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)\n",
    "    $$\n",
    "\n",
    "- **優化器：**\n",
    "\n",
    "  - 常用優化器如 SGD、Adam、RMSProp。\n",
    "\n",
    "- **學習率調整：**\n",
    "\n",
    "  - 使用學習率衰減、預熱等策略。\n",
    "\n",
    "- **數據增強：**\n",
    "\n",
    "  - 對訓練數據進行隨機變換，如旋轉、翻轉、裁剪，增加數據多樣性。\n",
    "\n",
    "- **正則化：**\n",
    "\n",
    "  - L2 正則化、Dropout、防止過擬合。\n",
    "\n",
    "---\n",
    "\n",
    "#### **10. 深入理解 CNN**\n",
    "\n",
    "- **特徵可視化：**\n",
    "\n",
    "  - 觀察卷積核和特徵圖，理解模型學習到的模式。\n",
    "\n",
    "- **感受野（Receptive Field）：**\n",
    "\n",
    "  - 理解網路中每個神經元對輸入的感知範圍。\n",
    "\n",
    "- **遷移學習：**\n",
    "\n",
    "  - 使用預訓練模型，在新任務上進行微調，提高效率。\n",
    "\n",
    "---\n",
    "\n",
    "### **總結**\n",
    "\n",
    "卷積神經網路（CNN）通過卷積、激活、池化等操作，有效地處理高維度的圖像數據，提取不同層次的特徵。其優勢在於參數高效、能夠捕捉空間結構、具有平移不變性等。\n",
    "\n",
    "理解 CNN 的結構和原理，有助於你在計算機視覺領域開發強大的模型，應對各種複雜的任務。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
