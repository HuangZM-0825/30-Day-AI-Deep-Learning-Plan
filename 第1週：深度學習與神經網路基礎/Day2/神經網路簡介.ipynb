{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第二天的學習中，我們將深入理解神經網路中的 **反向傳播（Backpropagation）**、**激活函數（Activation Function）** 和 **優化器（Optimizer）**。這些概念是神經網路能夠學習並提高準確性的關鍵部分。\n",
    "\n",
    "### 1. **反向傳播（Backpropagation）**\n",
    "反向傳播是訓練神經網路的核心演算法。它的目標是通過調整網路中的權重來最小化損失函數。反向傳播的過程可以分為兩個主要步驟：前向傳播和反向傳播。\n",
    "\n",
    "#### **前向傳播：**\n",
    "在前向傳播過程中，輸入數據從網路的輸入層通過隱藏層一直到達輸出層，並計算每一層的輸出值。這過程中會使用到神經元的激活函數。\n",
    "\n",
    "#### **反向傳播：**\n",
    "反向傳播通過計算損失函數對每一層權重的偏導數（即梯度）來更新權重，使損失函數最小化。具體步驟如下：\n",
    "1. 計算損失函數的值（例如均方誤差或交叉熵）。\n",
    "2. 通過反向傳遞誤差來計算每一層權重的梯度。\n",
    "3. 使用優化器來更新權重，使誤差逐漸減小。\n",
    "\n",
    "損失函數的梯度使用鏈式法則計算，這就是為什麼反向傳播的名字中包含「傳播」這個詞。\n",
    "\n",
    "**梯度計算公式：**\n",
    "假設 $L$ 是損失函數，$w$ 是網路的權重，則每一層的權重更新規則如下：\n",
    "$$\n",
    "w = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "其中：\n",
    "- $\\eta$ 是學習率（learning rate），決定了每次權重更新的步長。\n",
    "- $\\frac{\\partial L}{\\partial w}$ 是損失函數對權重的偏導數。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **激活函數（Activation Function）**\n",
    "\n",
    "激活函數決定了每個神經元的輸出，它引入了非線性，使得神經網路能夠學習複雜的模式。沒有激活函數，神經網路只是一個線性變換，無法處理複雜的問題。\n",
    "\n",
    "#### **常見的激活函數：**\n",
    "\n",
    "- **Sigmoid 函數：**\n",
    "  Sigmoid 函數將輸入映射到 $(0, 1)$ 之間，常用於二分類問題的輸出層。\n",
    "  $$ \n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}} \n",
    "  $$\n",
    "\n",
    "- **ReLU（Rectified Linear Unit）：**\n",
    "  ReLU 是當前最常用的激活函數之一。它的輸出為非負數，能夠有效解決梯度消失問題。\n",
    "  $$ \n",
    "  f(x) = \\max(0, x) \n",
    "  $$\n",
    "\n",
    "- **Tanh 函數：**\n",
    "  Tanh 函數將輸入值映射到 $(-1, 1)$ 之間，適用於輸入具有正負範圍的情況。\n",
    "  $$ \n",
    "  \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \n",
    "  $$\n",
    "\n",
    "不同的激活函數對網路性能的影響非常大，ReLU 是目前應用最廣泛的激活函數，因為它的計算效率較高，且在深層網路中能夠減少梯度消失的問題。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **優化器（Optimizer）**\n",
    "\n",
    "優化器是用來更新神經網路中權重的算法，通過最小化損失函數來提高模型的準確性。常見的優化器有以下幾種：\n",
    "\n",
    "#### **常見優化器：**\n",
    "\n",
    "- **隨機梯度下降（Stochastic Gradient Descent, SGD）：**\n",
    "  SGD 每次只使用一個樣本來更新權重，更新速度較快，但更新過程可能會有較大的波動。\n",
    "  $$ \n",
    "  w = w - \\eta \\cdot \\nabla_w L \n",
    "  $$\n",
    "\n",
    "- **動量（Momentum）：**\n",
    "  動量是在 SGD 基礎上引入一個動量項，幫助模型更快地收斂。\n",
    "  $$ \n",
    "  v = \\gamma v + \\eta \\cdot \\nabla_w L \\quad , \\quad w = w - v \n",
    "  $$\n",
    "  其中，$\\gamma$ 是動量係數。\n",
    "\n",
    "- **Adam（Adaptive Moment Estimation）：**\n",
    "  Adam 是目前最常用的優化器之一，它結合了動量和自適應學習率的思想。Adam 能夠自動調整學習率，從而提高收斂速度。\n",
    "  $$ \n",
    "  m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_w L \n",
    "  $$\n",
    "  $$ \n",
    "  v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_w L)^2 \n",
    "  $$\n",
    "  其中，$m_t$ 和 $v_t$ 是梯度的動量和平方梯度，$\\beta_1$ 和 $\\beta_2$ 是超參數。\n",
    "\n",
    "#### **優化器選擇：**\n",
    "不同的優化器有不同的特性，通常推薦在大多數情況下使用 Adam，因為它具有自動調整學習率和較快的收斂速度。在模型訓練過程中，如果發現學習率的調整非常敏感，可以考慮使用 Adam。\n",
    "\n",
    "---\n",
    "\n",
    "### 總結\n",
    "- **反向傳播** 是一種通過計算梯度來更新權重的算法，它使神經網路能夠學習並最小化損失函數。\n",
    "- **激活函數** 為神經網路引入了非線性，使其能夠學習更複雜的模式。常見的激活函數有 Sigmoid、ReLU 和 Tanh。\n",
    "- **優化器** 決定了如何更新神經網路的權重，SGD 和 Adam 是常見的優化方法，Adam 具有較好的性能和靈活性。\n",
    "\n",
    "這些概念是訓練深度神經網路的重要組成部分，掌握它們後，將有助於更深入的學習和實作。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
