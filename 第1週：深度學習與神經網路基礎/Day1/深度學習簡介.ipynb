{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解深度學習的核心概念是非常重要的，這些概念構成了神經網路和深度學習模型的基礎。以下是關於感知器、梯度下降、損失函數和反向傳播的詳細解釋：\n",
    "\n",
    "### 1. **感知器（Perceptron）**\n",
    "\n",
    "感知器是最基本的神經網路單元，它接受多個輸入，通過一組權重進行加權求和，然後將結果傳遞給激活函數來決定輸出。\n",
    "\n",
    "感知器的運作方式可以用以下公式表示：\n",
    "\n",
    "$$\n",
    "y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n",
    "$$\n",
    "\n",
    "- $x_i$ 是輸入特徵。\n",
    "- $w_i$ 是權重。\n",
    "- $b$ 是偏差（bias）。\n",
    "- $f$ 是激活函數，例如 Sigmoid 或 ReLU 函數。\n",
    "\n",
    "如果輸出結果超過某個閾值（例如在 Sigmoid 函數中大於 0.5），則輸出為 1，否則為 0。\n",
    "\n",
    "感知器是最簡單的神經網路模型，但在解決複雜問題上有局限性，因此出現了多層感知器（MLP）和更複雜的深度神經網路。\n",
    "\n",
    "### 2. **梯度下降（Gradient Descent）**\n",
    "\n",
    "梯度下降是一種優化演算法，用來最小化模型的損失函數。它的基本思想是通過調整神經網路的權重，使模型的預測結果與真實值之間的誤差（損失）最小。\n",
    "\n",
    "梯度下降的工作原理：\n",
    "- 首先，隨機初始化權重。\n",
    "- 計算損失函數的梯度，即損失對每個權重的導數，表示損失如何隨著權重的改變而變化。\n",
    "- 根據梯度的方向更新權重，更新的方式為：\n",
    "\n",
    "$$\n",
    "w = w - \\eta \\cdot \\nabla_w L\n",
    "$$\n",
    "\n",
    "- $\\eta$ 是學習率（learning rate），決定每次更新的步長。\n",
    "- $\\nabla_w L$ 是損失函數對權重的梯度。\n",
    "\n",
    "#### 梯度下降的種類：\n",
    "- **批量梯度下降（Batch Gradient Descent）：** 使用整個訓練集來計算一次梯度更新，速度較慢但較穩定。\n",
    "- **隨機梯度下降（Stochastic Gradient Descent, SGD）：** 每次只用一個樣本進行權重更新，速度快但不穩定。\n",
    "- **小批量梯度下降（Mini-batch Gradient Descent）：** 使用一小部分訓練集進行更新，速度與穩定性之間取得折衷。\n",
    "\n",
    "### 3. **損失函數（Loss Function）**\n",
    "\n",
    "損失函數用來衡量模型預測與實際值之間的差距，目的是最小化這個損失函數，使模型的預測結果更接近真實值。\n",
    "\n",
    "常見的損失函數包括：\n",
    "- **均方誤差（Mean Squared Error, MSE）：** 用於回歸問題，計算預測值與真實值之間的平方差。\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "- **交叉熵損失（Cross-Entropy Loss）：** 用於分類問題，特別是多分類問題。它衡量預測的概率分佈與真實分佈之間的差異。\n",
    "\n",
    "$$\n",
    "L = - \\sum_{i=1}^{n} y_i \\log(\\hat{y_i})\n",
    "$$\n",
    "\n",
    "- $y_i$ 是實際標籤。\n",
    "- $\\hat{y_i}$ 是預測的概率。\n",
    "\n",
    "### 4. **反向傳播（Backpropagation）**\n",
    "\n",
    "反向傳播是一種用於訓練神經網路的算法，通過更新權重來最小化損失。\n",
    "\n",
    "反向傳播的步驟：\n",
    "1. **前向傳播：** 根據當前的權重，將輸入數據通過各層神經網路傳遞，最終計算出模型的預測值。\n",
    "2. **計算損失：** 使用損失函數來衡量模型的預測值與真實值之間的誤差。\n",
    "3. **反向傳播：** 根據損失對每一層權重的偏導數（梯度）逐層計算，從輸出層開始，逐層向前傳遞，計算每一層的權重和偏差的梯度。\n",
    "4. **更新權重：** 使用梯度下降法，根據計算出的梯度，調整每一層的權重：\n",
    "\n",
    "$$\n",
    "w = w - \\eta \\cdot \\nabla_w L\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 總結\n",
    "- **感知器** 是神經網路的基本構建單元。\n",
    "- **梯度下降** 是一種用來調整權重以最小化損失函數的優化算法。\n",
    "- **損失函數** 衡量模型預測結果與實際值之間的差距。\n",
    "- **反向傳播** 是更新神經網路權重的過程，使用梯度下降法來使損失最小化。\n",
    "\n",
    "這些概念是深度學習的核心，在訓練與優化神經網路時非常重要。理解它們後，你將更容易掌握更複雜的深度學習模型和應用。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
