{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow 實作\n",
    "\n",
    "使用 TensorFlow 來建立一個簡單的 MLP 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 下載並預處理 MNIST 資料集\n",
    "# MNIST 是一個手寫數字的資料集，包含 60,000 張訓練圖像和 10,000 張測試圖像\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# 將圖像數據從 0-255 的範圍縮放到 0-1 之間，這樣可以加速模型的訓練\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 創建模型\n",
    "# 使用 Sequential 模型，這是一個線性堆疊的模型\n",
    "model = models.Sequential([\n",
    "    # 將 28x28 的圖像展開成一維向量，這樣可以輸入到全連接層中\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    # 全連接層，包含 128 個神經元，使用 ReLU 激活函數\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    # Dropout 層，隨機丟棄 20% 的神經元，防止過擬合\n",
    "    layers.Dropout(0.2),\n",
    "    # 輸出層，包含 10 個神經元，對應 10 個類別，使用 softmax 激活函數\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 編譯模型\n",
    "# 使用 Adam 優化器，損失函數為 sparse_categorical_crossentropy，評估指標為準確率\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型\n",
    "# 使用訓練數據進行訓練，訓練 5 個 epoch\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# 評估模型\n",
    "# 使用測試數據進行評估，並輸出測試準確率\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch 實作\n",
    "接下來，我們使用 PyTorch 實作同樣的 MLP 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 設置資料轉換\n",
    "# 將圖像轉換為張量並標準化到 [-1, 1] 範圍\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# 下載並加載 MNIST 資料集\n",
    "# MNIST 是一個手寫數字的資料集，包含 60,000 張訓練圖像和 10,000 張測試圖像\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 使用 DataLoader 將資料集分批次加載\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# 創建神經網路模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # 將 28x28 的圖像展開成一維向量\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 全連接層，將 784 維輸入轉換為 128 維\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        # Dropout 層，隨機丟棄 20% 的神經元，防止過擬合\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # 全連接層，將 128 維輸入轉換為 10 維，對應 10 個類別\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 前向傳播\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # 使用 softmax 激活函數，將輸出轉換為概率分佈\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "# 初始化模型、損失函數與優化器\n",
    "model = MLP()\n",
    "# 使用交叉熵損失函數\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 使用 Adam 優化器，學習率為 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 訓練模型\n",
    "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()  # 設置模型為訓練模式\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "            outputs = model(images)  # 前向傳播\n",
    "            loss = criterion(outputs, labels)  # 計算損失\n",
    "            loss.backward()  # 反向傳播\n",
    "            optimizer.step()  # 更新參數\n",
    "            running_loss += loss.item()  # 累加損失\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")  # 輸出每個 epoch 的平均損失\n",
    "\n",
    "# 測試模型\n",
    "def test(model, test_loader):\n",
    "    model.eval()  # 設置模型為評估模式\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # 禁用梯度計算\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)  # 前向傳播\n",
    "            _, predicted = torch.max(outputs.data, 1)  # 獲取預測結果\n",
    "            total += labels.size(0)  # 累加總樣本數\n",
    "            correct += (predicted == labels).sum().item()  # 累加正確預測數\n",
    "    print(f'Test Accuracy: {100 * correct / total}%')  # 輸出測試準確率\n",
    "\n",
    "# 執行訓練與測試\n",
    "train(model, train_loader, criterion, optimizer, epochs=5)\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **步驟說明：**\n",
    "\n",
    "1. **資料處理：**\n",
    "   - 我們從 TensorFlow 和 PyTorch 內建的 `MNIST` 資料集下載手寫數字圖像資料，並對它們進行標準化（將像素值縮放至 0-1 之間）。\n",
    "   \n",
    "2. **神經網路結構：**\n",
    "   - 在 TensorFlow 和 PyTorch 中，我們建立了一個簡單的多層感知器（MLP）。首先將 28x28 的圖像展開成一個向量（784 維），接著通過一個隱藏層進行全連接，並應用 `ReLU` 激活函數。最後的輸出層有 10 個神經元，對應於 0-9 的手寫數字分類。\n",
    "\n",
    "3. **訓練與評估：**\n",
    "   - 模型會訓練 5 個 epochs，並對測試集進行評估。你將得到模型的準確度。\n",
    "\n",
    "這是基本的 MLP 神經網路，未來的訓練我們將會進一步探索更複雜的架構，如卷積神經網路（CNN）和 YOLO 物件檢測模型。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
